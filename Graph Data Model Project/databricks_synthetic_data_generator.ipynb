{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator \n",
    "This notebook generates synthetic data in a Databricks notebook. This notebook mimicks work completed while serving in a position and replaces all PII and confidential data with dummy data. The data will not run unless connected to a real databricks account, storage account, neo4j instance, and corresponding code is updated to reflect so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dbac35c-138d-4f24-8187-7e299208db78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuring access credentials and endpoints for interacting with Azure services and Neo4j, within Databricks. \n",
    "\n",
    "AUTHORITY = f\"https://login.microsoftonline.us/dummyaccount.onmicrosoft.com\"\n",
    "NEO4J_APPLICATION_ID = \"0000000-0000-0000-0000-0000000\"\n",
    "SP_CLIENT_ID = \"0000000-0000-0000-0000-0000000\"\n",
    "SP_CLIENT_SECRET = dbutils.secrets.get(scope = \"NeoSecretScope\", key = \"ABC-DEVTEST-ABC-DEF\")\n",
    "\n",
    "STORAGE_ACCOUNT_URL = \"https://mystorageaccount.dfs.core.windows.net/my-container\"\n",
    "CONTAINER = \"published\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f668d5a9-f473-4b4d-a3ea-872d58ed0a83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries \n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import datetime\n",
    "import numpy as np  \n",
    "import os\n",
    "import random\n",
    "import string\n",
    "from delta import configure_spark_with_delta_pip\n",
    "from azure.storage.blob import BlobServiceClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2521a9b1-b9bf-4da3-abbf-dde9a67c3832",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initializes a Spark session named SyntheticDataGenerator with Delta Lake support using Databricks' Delta configuration helper for reading/writing Delta tables and to ensure compatibility outside of Databricks.\n",
    "\n",
    "builder = (\n",
    "    SparkSession.builder.appName(\"SyntheticDataGenerator\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    ")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "451bde0a-e8b7-4db8-84ef-2724bbfd97dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "BASE_PATH = f\"abfss://{CONTAINER}@{STORAGE_ACCOUNT_URL}\"\n",
    "\n",
    "# Record Counts\n",
    "N_ABVR = 15 \n",
    "N_ORG_ID = 200 \n",
    "N_TASK_ID = 1000 \n",
    "N_COMPLETE_ID = 1000 \n",
    "\n",
    "# ID ranges\n",
    "ID_RANGES = {\n",
    "   'ORG_COMPL_ID_START': 200, \n",
    "   'TASK_ID_START': 600,\n",
    "}\n",
    "\n",
    "ORG_COMPL_ID_START = 200 \n",
    "TASK_ID_START = 600\n",
    "\n",
    "# Value lists\n",
    "FILTER_ABC = ['0', '1', '2']\n",
    "\n",
    "# Defined IDs\n",
    "def generate_abvr_id(n, seed=42):\n",
    "    random.seed(seed)\n",
    "    return ['ABC-005' + ''.join(random.choices(string.ascii_uppercase, k=1)) for _ in range(n)]\n",
    "\n",
    "def generate_org_id(n, seed=42):\n",
    "    random.seed(seed) \n",
    "    return [f\"{i}\" for i in range(ORG_COMPL_ID_START, ORG_COMPL_ID_START + n)]\n",
    "\n",
    "def generate_task_ids(n, seed=42):\n",
    "    random.seed(seed) \n",
    "    return [f\"{i}\" for i in range(TASK_ID_START, TASK_ID_START + n)]\n",
    "\n",
    "def generate_task_dot_num(n, seed):\n",
    "    random.seed(seed) \n",
    "    return ['.'.join(map(str, random.sample(range(1,18), random.randint(2, 5)))) for _ in range(n)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61d1f9ed-bbda-4498-8e1f-4692087fcda5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Generate Organization Abbreviation data from flatfile\n",
    "def generate_abvr_data(n_rows= N_ABVR):\n",
    "    data = {\n",
    "        'ABVR_ABVR': [''.join(random.choices(string.ascii_uppercase, k=3)) for i in range(N_ABVR)],\n",
    "        'ORG_ABVR_ID': generate_abvr_id(N_ABVR)\n",
    "}\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Generate and save\n",
    "df = generate_abvr_data()\n",
    "print(df)\n",
    "spark_df = spark.createDataFrame(df)\n",
    "abvr_path = f\"{BASE_PATH}/abvr\"\n",
    "spark_df.write.mode(\"overwrite\").format(\"delta\").save(abvr_path)\n",
    "\n",
    "# Verify\n",
    "print(\"Org Abbreviation Data Generated:\")\n",
    "display(spark.read.format(\"delta\").load(abvr_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01e0e190-826c-4bb0-96cb-fe4a9317b3d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Generate Division data from flatfile\n",
    "def generate_org_data(n_rows=N_ORG_ID):\n",
    "    data = {\n",
    "        'ID_ORG': generate_org_id(n_rows),\n",
    "        'ORG_ABVR_ID': generate_abvr_id(n_rows),\n",
    "        'FILTER_ABC': [random.choice(FILTER_ABC) for _ in range(n_rows)]\n",
    "    }\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Generate and save\n",
    "df = generate_org_data()\n",
    "print(df)\n",
    "spark_df = spark.createDataFrame(df)\n",
    "org_path = f\"{BASE_PATH}/org\"\n",
    "spark_df.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").format(\"delta\").save(org_path)\n",
    "\n",
    "# Verify\n",
    "print(\"Division Data Generated:\")\n",
    "display(spark.read.format(\"delta\").load(org_path)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c07893b3-995a-4514-bce5-16ea4c0278ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. Generate Task Completion data mimicks data from SharePoint list\n",
    "def generate_complete_data(n_rows=N_COMPLETE_ID):\n",
    "    compl_ids = list(range(ORG_COMPL_ID_START, ORG_COMPL_ID_START + n_rows)) \n",
    "    org_ids = generate_org_id(n_rows, seed=42)\n",
    "    org_names = [f\"Org Name for Org {id}\" for id in org_ids]\n",
    "    task_ids = generate_task_ids(n_rows)\n",
    "    \n",
    "    data = {\n",
    "        'Organizations': [', '.join(random.sample(org_names, random.randint(1, 5))) for _ in range(n_rows)],\n",
    "        'task_title_choice': [', '.join(random.sample(task_ids, random.randint(1, 5))) for _ in range(n_rows)],\n",
    "        'ID': compl_ids\n",
    "    }\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Generate and save\n",
    "df = generate_complete_data()\n",
    "spark_df = spark.createDataFrame(df)\n",
    "completion_path = f\"{BASE_PATH}/completion\"\n",
    "spark_df.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").format(\"delta\").save(completion_path)\n",
    "\n",
    "# Verify\n",
    "print(\"Task Completion Data Generated:\")\n",
    "display(spark.read.format(\"delta\").load(completion_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76bc4b09-d289-42b0-b2bb-bdf76f09f286",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. Generate Task Information data mimicks data from SharePoint list\n",
    "def generate_task_data(n_rows=N_TASK_ID):\n",
    "    data = {\n",
    "        'ID': generate_task_ids(n_rows, seed=41),\n",
    "        'task_ID': generate_task_dot_num(n_rows, seed=40), \n",
    "        'task_title': [f\"{task} Detailed DEF task Title for task IDs {task_id}\" for task, task_id in zip(generate_task_dot_num(n_rows, seed=40), generate_task_ids(n_rows))]\n",
    "    }\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Generate and save\n",
    "df = generate_task_data()\n",
    "spark_df = spark.createDataFrame(df)\n",
    "task_path = f\"{BASE_PATH}/task\"\n",
    "spark_df.write.mode(\"overwrite\").format(\"delta\").save(task_path)\n",
    "\n",
    "# Verify\n",
    "print(\"Task Information Data Generated:\")\n",
    "display(spark.read.format(\"delta\").load(task_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0225f0a0-82d6-4db5-98da-d54fc442e943",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5 Generate Crosswalk Task data from flatfile\n",
    "def generate_crosswalk_task_data(n_rows=N_TASK_ID):\n",
    "    task_dot_num = generate_task_dot_num(n_rows, seed= 40)\n",
    "    task_np_dot_num = generate_task_dot_num(n_rows, seed= 43)\n",
    "\n",
    "    data = {\n",
    "        'task_p_hier_id': task_dot_num,\n",
    "        'task_p_hier_nm':[f\"Detailed DEF task Title for task IDs {task}\" for task in task_dot_num],\n",
    "        'task_hier_id': task_np_dot_num,\n",
    "        'task_hier_nm':[f\"Detailed OPTION task Title for task IDs {task}\" for task in task_np_dot_num],\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "df = generate_crosswalk_task_data()\n",
    "spark_df = spark.createDataFrame(df)\n",
    "crosswalk_task_path = f\"{BASE_PATH}/crosswalk_task\"\n",
    "spark_df.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").format(\"delta\").save(crosswalk_task_path)\n",
    "\n",
    "# Verify\n",
    "print(\"Crosswalk Task Data Generated:\")\n",
    "display(spark.read.format(\"delta\").load(crosswalk_task_path))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "dapm_databricks_synthetic_data_generator",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
