{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87632a73-97b8-478c-81f6-defc4ad1c815",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Task Ingest\n",
    "\n",
    "This notebook reads source file from Azure Data Lake Storage Account (ADLS) and updates TASK graph in Neo4j. This notebook mimicks work completed while serving in a position and replaces all PII and confidential data with dummy data. The data will not run unless connected to a real databricks account, storage account, neo4j instance, and corresponding code is updated to reflect so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74c92932-8604-4fdd-9bdf-228f1915a515",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b3242e5-6efd-4365-818a-22f88ae980d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuring access credentials and endpoints for interacting with Azure services and Neo4j, within Databricks. \n",
    "\n",
    "AUTHORITY = f\"https://login.microsoftonline.us/dummyaccount.onmicrosoft.com\"\n",
    "NEO4J_APPLICATION_ID = \"0000000-0000-0000-0000-0000000\"\n",
    "SP_CLIENT_ID = \"0000000-0000-0000-0000-0000000\"\n",
    "SP_CLIENT_SECRET = dbutils.secrets.get(scope = \"NeoSecretScope\", key = \"ABC-DEVTEST-ABC-DEF\")\n",
    "\n",
    "STORAGE_ACCOUNT_URL = \"https://mystorageaccount.dfs.core.windows.net/my-container\"\n",
    "CONTAINER = \"published\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries \n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import datetime\n",
    "import numpy as np  \n",
    "import os\n",
    "import random\n",
    "import string\n",
    "from delta import configure_spark_with_delta_pip\n",
    "from azure.storage.blob import BlobServiceClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72f9966e-201c-49e8-951d-0d660a56094e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create Intermediate Dataframes\n",
    "In order to ingest to graph it is necessary to create an intermediate dataframes from source tables.\n",
    "\n",
    "In this notebook an intermediate table that renames ID to DEF ID and splits the Applicable Oranization (Rename new column to DEF_Org_Name) and Task_Table_Choice (Rename new column to ID) columns in the completion table is created \n",
    "\n",
    "An intermediate table that connects abv_abvr to org (on OPTION Abvr_ID) to the new completion table (on DEF_ORG_Name) is created\n",
    "\n",
    "An intermediate table that connects the task to crosswalk_task to dftask_task table is created\n",
    "\n",
    "An intermediate table that counts the number of times a DEF_Org_Name completion an ID is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0693f356-22ea-4926-9a10-97ccfa86ebdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Helps with Delta features like generated columns or writer versioning in case columns are renamed or reordered\n",
    "spark.conf.set(\"spark.databricks.delta.columnMapping.mode\", \"name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b526e1e3-ee95-408b-9f74-09a83a7cd460",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## List Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f255e13-3dbb-416d-8bf1-669d41f1a054",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "BASE_PATH = f\"abfss://{CONTAINER}@{STORAGE_ACCOUNT_URL}\"\n",
    "\n",
    "tables_df = spark.createDataFrame(dbutils.fs.ls(BASE_PATH))\n",
    "\n",
    "display(tables_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "832af783-2b08-46d7-a579-6f91c0862503",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Intermediate dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd3de4b1-3b8c-4db9-8889-2f78bf109115",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Intermediate table that renames ID to DEF ID and splits the Applicable Organization (Rename new column to DEF_Org_Name) and Task_Title_Choice (Rename new column to ID) columns in the completion table is created \n",
    "\n",
    "completion_df = spark.read.format(\"delta\").load(BASE_PATH + \"/\" + \"completion/\")\n",
    "\n",
    "completion_split_df = completion_df.withColumn(\"Organizations\", split(col(\"Organizations\"), \";\")).withColumn(\"Task Title Choice\", split(col(\"Task Title Choice\"), \";\"))\n",
    "\n",
    "completion_explode_df = completion_split_df.withColumn(\"Organizations\", explode(col(\"Organizations\"))).withColumn(\"Task Title Choice\", explode(col(\"Task Title Choice\")))\n",
    "\n",
    "completion_clean_df = completion_explode_df.filter(~col(\"Organizations\").rlike(r\"#\\d{3}\"))\n",
    "\n",
    "completion_filter_df = completion_clean_df.withColumn(\"Organizations\", regexp_replace(col(\"Organizations\"), r\"^#\", \"\")).withColumn(\"Task Title Choice\", regexp_replace(col(\"Task Title Choice\"), r\"^#\", \"\"))\n",
    "\n",
    "#Change Applicable_Organizations to DEF_Org_Name, to match org. Change ID to Complete_Num to match, so that the label Task_Title Choice could be changed to ID\n",
    "completion_new_df = completion_filter_df.withColumnRenamed(\"Organizations\", \"DEF Org Name\").withColumnRenamed(\"ID\", \"Completion Num\")\n",
    "\n",
    "#Change Task_Title_Choice to ID, which is the Task ID\n",
    "completion_newest_df = completion_new_df.withColumnRenamed(\"Task Title Choice\", \"ID\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd48d2a4-8e50-44c0-bd0e-a1277284b2f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Intermediate table that connects abvr to org (on OPTION_Abvr_ID) to the new completion table (on DEF_ORG_Name) is created\n",
    "\n",
    "#Load the exisiting tables \n",
    "abvr_df = spark.read.format(\"delta\").load(BASE_PATH+\"/\"+\"abvr/\")\n",
    "\n",
    "org_df = spark.read.format(\"delta\").load(BASE_PATH+\"/\"+\"org/\")\n",
    "org_df = org_df.filter(col('ID_ORG').isNotNull())\n",
    "\n",
    "#Join abvabv table and org table on the 'OPTION_Abvr_ID' field\n",
    "abv_org_df = org_df.join(abvr_df, on= \"OPTION Abvr ID\", how=\"outer\")\n",
    "abvrorg_completion_df = completion_new_df.join(abv_org_df, on=\"DEF Org Name\", how=\"outer\")\n",
    "\n",
    "for col_name in abvrorg_completion_df.columns:\n",
    "    abvrorg_completion_df = abvrorg_completion_df.withColumnRenamed(col_name, col_name.replace(\" \", \"_\"))\n",
    "\n",
    "abvrorg_update_df = abvrorg_completion_df\n",
    "\n",
    "display(abvrorg_update_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15c5ecc2-3aad-4395-9912-e62ca80cd137",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Intermediate table that connects crosswalk_task to task_path and then to task table is created\n",
    "#Load the exisiting tables \n",
    "\n",
    "crosswalk_task_df = spark.read.format(\"delta\").load(BASE_PATH+\"/\"+\"crosswalk_task_path\")\n",
    "crosswalk_task_df = crosswalk_task_df.filter(col('task_p_hier_id').rlike('^(\\d+(?:\\.\\d+)*)'))\n",
    "\n",
    "crosswalk_df = crosswalk_task_df.drop(\"task_hier_nm\", \"task_hier_txt\")\n",
    "\n",
    "task_df = spark.read.format(\"delta\").load(BASE_PATH+\"/\"+\"task_path/\")\n",
    "task_df = task_df.withColumn('Task ID', trim(col('Task ID')))\n",
    "task_df = task_df.withColumn('Task ID', regexp_replace(\"Task ID\", \"-\", \"\"))\n",
    "task_df = task_df.filter(col('Task ID').rlike('^(\\d+(?:\\.\\d+)*)'))\n",
    "\n",
    "#Then join task_crosswalk_df to task_path and rename tasks columns (Task ID to task_p_hier_id)\n",
    "#Rename columns\n",
    "task_renamed_df = task_df.withColumnRenamed(\"Task ID\", \"task_p_hier_id\")\n",
    "\n",
    "task_renamed_df = task_renamed_df.withColumn(\"task_p_hier_id\", trim(col(\"task_p_hier_id\")))\n",
    "\n",
    "task_crosswalk_df = task_crosswalk_df.withColumn(\"task_p_hier_id\", trim(col(\"task_p_hier_id\")))\n",
    "\n",
    "functions_df = task_renamed_df.join(task_crosswalk_df, on=\"task_p_hier_id\", how=\"outer\")\n",
    "display(functions_df)\n",
    "\n",
    "# Resolves bug SDP-37858\n",
    "functions_drop_df = functions_df.where(~functions_df[\"ID\"].contains(\"Both voice\"))\n",
    "\n",
    "functions_update_df = functions_drop_df.where(col(\"ID\").isNotNull()).withColumn(\"ID\", col(\"ID\").cast(IntegerType()))\n",
    "display(functions_update_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1baab89f-2d59-4949-b66d-8bbb3e7d1538",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#An intermediate table that counts the number of times a DEF_Org_Name completion an ID is created\n",
    "performed_df = abvrorg_update_df.withColumnRenamed(\"Task_Title_Choice\", \"ID\")\n",
    "display(performed_df)\n",
    "\n",
    "#Updated to resolve bug SDP-37887\n",
    "Task_count = performed_df.groupBy(\"DEF_Org_Name\", \"ID\").count()\n",
    "display(Task_count.orderBy(\"DEF_Org_Name\",\"ID\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9e6b61d-6fcd-446e-b28c-cf732b359274",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## NEO4J Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97cc12b0-653e-4c91-90cb-45a9e40eec5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa0f2803-7b41-4cbf-b22a-213db454d970",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the Cypher query to use in the write \n",
    "\n",
    "organization_df = abvrorg_update_df.filter(col('ABVR').isNotNull())\n",
    "\n",
    "# ['ABVR', 'Abvr_NAME', 'OPTION_Abvr_ID']\n",
    "organization_query = \"MERGE (n:Organization {id: event.ABVR}) SET n.OPTIONAbvrID = event.OPTION_Abvr_ID, n.AbvrName = event.Abvr_NAME\"\n",
    "\n",
    "# ['DEF_Org_Name', 'Abvr', 'ABVR', 'Filter_ABC', 'ID_ORG']\n",
    "DEForg_query = \"MERGE (n:DEFOrg {id: event.DEF_Org_Name}) SET n.Abvr = event.ABVR, n.ABCFilter = event.Filter_ABC, n.OrgID = event.ID_ORG\"\n",
    "\n",
    "# ['task_p_hier_nm', 'task_p_hier_id', 'ID']\n",
    "function_query = \"MERGE (n:Function {id: event.ID}) SET n.DEFTaskName = event.task_p_hier_nm, n.DEFTaskID = event.task_p_hier_id\"\n",
    "\n",
    "nodes :dict = {\n",
    "    \"Organization\" : (organization_df, organization_query), \n",
    "    \"DEForg\" : (abvrorg_update_df, DEForg_query), \n",
    "    \"Function\" : (functions_update_df, function_query),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c8a6d60-f5a8-4b06-8c4e-6839c63f23ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Load Nodes\n",
    "\n",
    "for label in nodes.keys(): \n",
    "    query = nodes[label][1]\n",
    "    df = nodes[label][0]\n",
    "\n",
    "    df.write.format(\"org.neo4j.spark.DataSource\").option(\"query\", query).mode(\"Overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db0d5d27-3085-4442-8c88-57572fb12045",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48e4971d-c708-447b-8ae8-92f86b2a1403",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Task_count_2 = Task_count.withColumn(\"ID\", col(\"ID\").cast(IntegerType())).filter(col(\"ID\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5db8fc9a-f4ab-411e-adf7-5094268896ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(Task_count_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6f1fd71-df07-409a-af0b-a0112a5eed20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Queries to load relationships\n",
    "performed_query = \"\"\"\n",
    "MATCH (n:DEFOrg {id: event.DEF_Org_Name})\n",
    "MATCH (m:Function {id: event.ID})\n",
    "MERGE (n)-[r:PERFORMED {Task_count: event.count}]->(m)\n",
    "\"\"\"\n",
    "\n",
    "belongs_query = \"\"\"\n",
    "MATCH (n:Organization {id: event.ABVR})\n",
    "MATCH (m:DEFOrg {Abvr: event.ABVR})\n",
    "MERGE (m)-[b:BELONGS_TO]->(n)\n",
    "\"\"\"\n",
    "\n",
    "relationships :dict = {\n",
    "    \"PERFORMED\" : (Task_count_2, performed_query),\n",
    "    \"BELONGS_TO\" : (abvrorg_update_df, belongs_query)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d12d49a6-97ab-490b-ba24-b500a6ceec2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load Relationships\n",
    "for label in relationships.keys():  \n",
    "    query = relationships[label][1]\n",
    "    df = relationships[label][0]\n",
    "\n",
    "    df.write.format(\"org.neo4j.spark.DataSource\").option(\"query\", query).mode(\"Overwrite\").save()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "brm_ingest",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
